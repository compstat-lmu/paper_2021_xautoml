{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Appendix B.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(dplyr)\n",
    "library(ggplot2)\n",
    "library(ggpubr)\n",
    "source(\"helper.r\")\n",
    "theme_set(theme_pubr(legend = \"none\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to folder, with folders for dataset results\n",
    "path = \"../data/runs/mlp/\"\n",
    "datasets = list.files(path)\n",
    "\n",
    "# create list with one list containing one dataframe per dataset\n",
    "data.list = lapply(datasets, function(data){\n",
    "  \n",
    "  data.path = paste0(path, data, \"/2_3_effects_and_trees/\")\n",
    "  objectives = list.files(data.path)\n",
    "  \n",
    "  for(i in 1:length(objectives)){\n",
    " \n",
    "    res = readRDS(paste0(data.path, objectives[i]))\n",
    "    df.sub = res$result[[1]]$eval\n",
    "    df.sub$objective = res$objective\n",
    "    \n",
    "    if(i == 1) df = df.sub\n",
    "    else df = rbind(df, df.sub)\n",
    "  }\n",
    "  \n",
    "  return(df)\n",
    "})\n",
    "names(data.list) = datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Table  7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relative confidence improvement on dataset level\n",
    "df.datasets.conf = create_table_datasets(data.list, \"SS_L2\", \"conf.rel\", 7)\n",
    "\n",
    "# count features with highest and lowest improvements (for Table 7)\n",
    "df.feat.low      = as.data.frame(df.datasets.conf %>% group_by(feature.1) %>% summarise(n()))\n",
    "df.feat.high     = as.data.frame(df.datasets.conf %>% group_by(feature) %>% summarise(n()))\n",
    "df.feat.conf     = left_join(df.feat.high, df.feat.low, by = c(\"feature\"= \"feature.1\"))\n",
    "\n",
    "df.datasets.conf[,c(\"mean\",\"sd\",\"mean.1\", \"mean.2\")] = round(df.datasets.conf[,c(\"mean\",\"sd\",\"mean.1\", \"mean.2\")]*100,0)\n",
    "names(df.datasets.conf)[4:7] = c(\"feat.high\", \"mean.high\", \"feat.low\", \"mean.low\")\n",
    "df.datasets.conf[,c(1:3,5,7)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Table 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relative negative loglikelihood improvement on dataset level\n",
    "df.datasets.nll  = create_table_datasets(data.list, \"SS_L2\", \"neg_loglik.rel\", 7)\n",
    "\n",
    "# count features with highest and lowest improvements (for Table 7)\n",
    "df.feat.low      = as.data.frame(df.datasets.nll %>% group_by(feature.1) %>% summarise(n()))\n",
    "df.feat.high     = as.data.frame(df.datasets.nll %>% group_by(feature) %>% summarise(n()))\n",
    "df.feat.nll      = left_join(df.feat.high, df.feat.low, by = c(\"feature\"= \"feature.1\"))\n",
    "\n",
    "df.datasets.nll[,c(\"mean\",\"sd\",\"mean.1\", \"mean.2\")] = round(df.datasets.nll[,c(\"mean\",\"sd\",\"mean.1\", \"mean.2\")]*100,0)\n",
    "names(df.datasets.nll)[4:7] = c(\"feat.high\", \"mean.high\", \"feat.low\", \"mean.low\")\n",
    "df.datasets.nll[,c(1:3,5,7)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Table 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.feat = left_join(df.feat.conf, df.feat.nll, by = \"feature\")\n",
    "names(df.feat) = c(\"hyperparameter\",\"#MC.high\", \"#MC.low\", \"#NLL.high\", \"#NLL.low\") \n",
    "df.feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse different objective functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Table 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = c(\"conf.rel\", \"conf.rel.opt.1\", \"neg_loglik.rel\")\n",
    "tab.obj = lapply(targets, function(target){\n",
    "            tab.l2 = create_table_features(data.list = data.list, objective = \"SS_L2\", target = target, depth = 7)[,1:2]\n",
    "            tab.ar = create_table_features(data.list = data.list, objective = \"SS_area\", target = target, depth = 7)[,1:2]\n",
    "            tab.sd = create_table_features(data.list = data.list, objective = \"SS_sd\", target = target, depth = 7)[,1:2]\n",
    "            tab.mu = create_table_features(data.list = data.list, objective = \"SS_mean\", target = target, depth = 7)[,1:2]\n",
    "            tab = left_join(left_join(tab.l2, tab.ar, by = \"feature\"), tab.sd, by = \"feature\")\n",
    "            tab[,2:5] = tab[,2:5]*100\n",
    "            names(tab) = c(\"hyperparameter\", \"l2\", \"area\", \"sd\". \"mean\")\n",
    "            tab\n",
    "        })\n",
    "names(tab.obj) = targets\n",
    "tab.obj\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Increased Confidence with more splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare data to plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data for CNAE-9 dataset for run 9\n",
    "path = \"../data/runs/mlp/\"\n",
    "dataset   = \"cnae-9\"\n",
    "iter      = 9\n",
    "data.all  = readRDS(paste0(path, dataset, \"/2_3_effects_and_trees/eval_SS_L2_20_1000.rds\"))\n",
    "testdata  = readRDS(paste0(path, dataset, \"/2_1_testdata/testdata_1000.rds\"))\n",
    "features  = unique(data.all$result[[1]]$feature)\n",
    "optima    = readRDS(paste0(path, dataset,\"/1_1_mlrmbo_runs/mlrmbo_run_lambda_1_30repls.rds\" ))\n",
    "traindata = optima$result[[iter]]$opt.path\n",
    "optimum   = traindata[which.min(traindata$y), features]\n",
    "gt        = readRDS(paste0(path, dataset, \"/2_2_groundtruth_pdps/gtpdp_20_1000.rds\"))\n",
    "#tree_data      = readRDS(paste0(path, dataset, \"/2_3_effects_and_trees/trees.rds\"))\n",
    "\n",
    "\n",
    "\n",
    "# Calculate PDP and confidence estimates for hyperparameter number of layers\n",
    "feat = \"num_layers\"\n",
    "\n",
    "# calculate global PDP and confidence bands\n",
    "data = data.all$result[[1]]$reslist[[iter]][[feat]]\n",
    "data.pdp = data$res.pdp\n",
    "data.pdp$lower = data.pdp$mean - data.pdp$sd*1.96\n",
    "data.pdp$upper = data.pdp$mean + data.pdp$sd*1.96\n",
    "\n",
    "# Extract ICE curves\n",
    "data.ice = data$res.ice\n",
    "\n",
    "# Extract tree and split criteria for optimal subregions\n",
    "tree = data$trees\n",
    "split.criteria = find_split_criteria(tree[[1]], optimum)\n",
    "split.criteria$values = round(split.criteria$values, 4)\n",
    "\n",
    "# Extract true PDP and ICE estimates\n",
    "gt.feat = gt[[1]][[feat]][[1]]\n",
    "gt.pdp = gt.feat[gt.feat$.type==\"pdp\",]\n",
    "gt.ice = gt.feat[gt.feat$.type == \"ice\",]\n",
    "\n",
    "# Calculate PDP and confidence estimates for optimal subregion after last split\n",
    "optimal.node = find_optimal_node(tree[[1]][1:7], optimum)\n",
    "ind.opt = optimal.node$subset.idx\n",
    "pdp.opt = data.ice[data.ice$.id %in% ind.opt,] %>% group_by(get(feat)) %>% summarise(mean.opt = mean(mean), sd.opt = mean(sd))\n",
    "pdp.opt$lower = pdp.opt$mean.opt - pdp.opt$sd.opt*1.96\n",
    "pdp.opt$upper = pdp.opt$mean.opt + pdp.opt$sd.opt*1.96\n",
    "colnames(pdp.opt) = c(feat, \"mean\", \"sd\",   \"lower\" , \"upper\")\n",
    "gt.opt = gt.ice[gt.ice$.id %in% ind.opt,] %>% group_by(get(feat)) %>% summarise(mean.opt = mean(mean))\n",
    "colnames(gt.opt) = c(feat, \"mean\")\n",
    "\n",
    "# Calculate PDP and confidence estimates for optimal subregion after second split\n",
    "optimal.node = find_optimal_node(tree[[1]][1:3], optimum)\n",
    "ind.opt.3 = optimal.node$subset.idx\n",
    "pdp.opt.3 = data.ice[data.ice$.id %in% ind.opt.3,] %>% group_by(get(feat)) %>% summarise(mean.opt = mean(mean), sd.opt = mean(sd))\n",
    "pdp.opt.3$lower = pdp.opt.3$mean.opt - pdp.opt.3$sd.opt*1.96\n",
    "pdp.opt.3$upper = pdp.opt.3$mean.opt + pdp.opt.3$sd.opt*1.96\n",
    "colnames(pdp.opt.3) = c(feat, \"mean\", \"sd\",   \"lower\" , \"upper\")\n",
    "gt.opt.3 = gt.ice[gt.ice$.id %in% ind.opt.3,] %>% group_by(get(feat)) %>% summarise(mean.opt = mean(mean))\n",
    "colnames(gt.opt.3) = c(feat, \"mean\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Figure 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_lim = c(-0.2, 0.8)\n",
    "\n",
    "# Global PDP with confidence bands\n",
    "p.pdp = ggplot(data = data.pdp, aes_string(x = feat, y = \"mean\")) + \n",
    "  geom_line(colour = \"blue\", lwd = 1) + \n",
    "  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.2) + \n",
    "  geom_line(data = gt.pdp, lwd = 1) +\n",
    "  geom_vline(aes(xintercept = as.numeric(optimum[feat])), colour = \"orange\", lwd = 1) +\n",
    "  ylim(y_lim) + xlab(\"Num. Layers\") + ylab(\"c\") +\n",
    "  labs(title = paste0(\"Entire HP Space, n = \", 1000)) +\n",
    "  theme(plot.caption = element_text(hjust = 0, face= \"italic\", size = 11),\n",
    "        plot.title = element_text(size = 11),\n",
    "        axis.title.y = element_text(size = 14))\n",
    "\n",
    "# Sub-regional PDP with confidence bands after split 6\n",
    "p.pdp.opt = ggplot(data = pdp.opt, aes_string(x = feat, y = \"mean\")) + \n",
    "  geom_line(colour = \"blue\", lwd = 1) + \n",
    "  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.2) + \n",
    "  geom_line(data = gt.opt, lwd = 1) +\n",
    "  geom_vline(aes(xintercept = as.numeric(optimum[feat])), colour = \"orange\", lwd = 1) +\n",
    "  xlab(\"Num. Layers\") + ylim(y_lim) + ylab(\"c\")  + \n",
    "  labs(title = paste0(\"Split 6: Opt. Sub-region, n = \", length(ind.opt))) +\n",
    "  theme(plot.caption = element_text(hjust = 0, face= \"italic\", size = 11),\n",
    "        plot.title = element_text(size = 11),\n",
    "        axis.title.y = element_text(size = 14))\n",
    "\n",
    "# Sub-regional PDP with confidence bands after split 2\n",
    "p.pdp.opt.3 = ggplot(data = pdp.opt.3, aes_string(x = feat, y = \"mean\")) + \n",
    "  geom_line(colour = \"blue\", lwd = 1) + \n",
    "  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.2) + \n",
    "  geom_line(data = gt.opt.3, lwd = 1) +\n",
    "  geom_vline(aes(xintercept = as.numeric(optimum[feat])), colour = \"orange\", lwd = 1) +\n",
    "  xlab(\"Num. Layers\") + ylim(y_lim) + ylab(\"c\")  + \n",
    "  labs(title = paste0(\"Split 2: Opt. Sub-region, n = \", length(ind.opt.3))) +\n",
    "  theme(plot.caption = element_text(hjust = 0, face= \"italic\", size = 11),\n",
    "        plot.title = element_text(size = 11),\n",
    "        axis.title.y = element_text(size = 14))\n",
    "\n",
    "gridExtra::grid.arrange(p.pdp, p.pdp.opt.3, p.pdp.opt, nrow = 1)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
