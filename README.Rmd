---
output: rmarkdown::github_document
---

```{r setup, echo=FALSE}
ggplot2::theme_set(ggplot2::theme_bw())
```

# Explaining Hyperparameter Optimization via PDPs

![](docs/images/tree_example.png)<!-- -->

This repository gives access to an implementation of the methods presented in the paper submission "Explaining Hyperparameter Optimization via PDPs", as well as all code that was used for the experimental analysis. 

This repository is structured as follows: 

```
    ├── analysis/               # Scripts used to create figures and tables in the paper
    ├── data/                   # Location where all experimental data is stored
    │   ├── raw/                # Raw datasets for the DNN surrogate benchmark
    │   ├── runs/               # Individual runs 
    ├── experiments/            # Code for experimental analysis (section 6)
    │   ├── synthetic           # Synthetic benchmark (section 6.1)
    │   ├── mlp                 # DNN surrogate benchmark (section 6.2)
    ├── renv/                   # renv configuration files to enable a reproducible setup 
    ├── R/                      # Implementation of methods 
    ├── LICENSE
    └── README.md               
```    

## Reproducible Setup 

To allow for a proper, reproducible setup of the environment we use the package `renv`. 

The project dependencies can be installed via 

```{r}
library("renv")
renv::restore()
```

## Quick Start  

```{r warning=FALSE}
# Loading all scripts we need
source("R/tree_splitting.R")
source("R/helper.R")
source("R/marginal_effect.R")
source("R/plot_functions.R")
```

First, assume we have a surrogate model that we want to analyze. 

Here, for example, we tuned a support vector machine on the `iris` task, and extracted the surrogate model after the last iteration. 

```{r warning=FALSE}
library(mlr)
library(mlrMBO)
library(e1071)

par.set = makeParamSet(
  makeNumericParam("cost", -15, 15, trafo = function(x) 2^x),
  makeNumericParam("gamma", -15, 15, trafo = function(x) 2^x)
)

ctrl = makeMBOControl()
ctrl = setMBOControlInfill(ctrl, crit = makeMBOInfillCritCB(cb.lambda = 1))
ctrl = setMBOControlTermination(ctrl, iters = 20)
tune.ctrl = makeTuneControlMBO(mbo.control = ctrl)
res = tuneParams(makeLearner("classif.svm"), iris.task, cv3, par.set = par.set, control = tune.ctrl,
  show.info = FALSE)
  
surrogate =  res$mbo.result$models[[1]]

print(surrogate)
```

We are computing the PDP estimate with confidence for hyperparameter `cost`. We use the `marginal_effect_sd_over_mean` function, which uses the `iml` packages. 

```{r warning=FALSE, echo=FALSE}
library(iml)

# Data used to compute the Monte Carlo estimate
data = generateRandomDesign(n = 100, par.set = par.set)

me = marginal_effect_sd_over_mean(model = surrogate, feature = "cost", data = data, method = "pdp_var")
head(me)
```

We visualize the outcome: 

```{r}
library(ggplot2)

p = plot_pdp_with_uncertainty_1D(me)
print(p)
```

To improve the uncertainty estimates, we partition the input space. We perform 3 splits and use the L2-objective. 

```{r warning = FALSE}
predictor = Predictor$new(model = surrogate, data = data)
effects = FeatureEffect$new(predictor = predictor, feature = "cost", method = "pdp")

tree = compute_tree(effects, data, "SS_L2", 5)
```

We now want to visualize the PDP in the node with the best objective after 3 splits. 

```{r warning = FALSE}
plot_pdp_for_node(tree)
```




## Reproduce Experiments 


The steps necessary to reproduce the experiments are described [here](benchmarks/README.Rmd).
