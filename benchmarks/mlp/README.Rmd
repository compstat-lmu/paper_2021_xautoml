---
output: rmarkdown::github_document
---

```{r setup}
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
```


# Experiments on a DNN Surrogate Benchmark (Section 6.2)

The setup of this experiment is slightly more complicated, since we first have to train an empirical performance model, before we can perform the MBO runs and perform the tree splitting. 


### (1) Training the Empirical Performance Model 

In order to train the empirical performance model, the LCBench data needs first to be downloaded from [here](https://ndownloader.figshare.com/files/21188598) and unzipped. 

Then, the following `get_data_from_LCBench.py` allows to store the respective datasets into a .csv files in a proper directory structure. The python script was tested with Python 3.7.6. 

As a result, you will find the LCBench data stored in `data/runs/mlp/<dataset>/0_objective/lcbench2000.csv`. 

Then, the following script allows will again setup the experimental registry in which the surrogates are computed. 

```{r}
source("benchmarks/mlp/1_empirical_performance_model/generate_data.R")
```

The above script create an experimental registry in `regs/LCBench_surogate_registry`. 

The core part, which is the actual definition of the experiment is in the function `randomsearch(...)` in the file `benchmarks/synthetic/config.R`. In this function, a basic random search is performed to find a proper configuration of a random forest, and fit a random forest with this configuration. 

Jobs are submitted as follows. 

```{r eval=FALSE}
reg = loadRegistry("regs/LCBench_surogate_registry", writeable = TRUE)

# Overview table over experiments to be run
tab = summarizeExperiments(
  by = c("job.id", "algorithm", "problem"))

# Submit all experiments to your local system  
submitJobs(tab)

# Alternatively, submit them to the linux cluster with respective resources
# source("benchmarks/resources.R")
# submitJobs(tab, resources = resources.serial.default)
```

The following commands will summarize the experimental results in a compact way and store it in 

```{r eval=FALSE}
reg = loadRegistry("regs/LCBench_surogate_registry")

# Check status of the experiments 
getStatus()

source("benchmarks/mlp/1_empirical_performance_model/reduce.R")
reduce_results_surrogate(reg, savedir = "data/runs/mlp_results")
```


### (2) Run MBO 

For each problem, i.e. for each dataset, we run mlrMBO to perform Bayesian optimization. We optimize the objective function as approximated by the empirical performance model as computed before. 

We perform 30 replications per experiment. 

```{r}
source("benchmarks/mlp/2_mbo_runs/generate_data.R")
```

The details of how the BO run is implemented can be seen in the function `mlrmbo(...)`.

Again, jobs are submitted and reduced. 

```{r eval=FALSE}
reg = loadRegistry("regs/mlp_bo_registry", writeable = TRUE)

# Overview table over experiments to be run
tab = summarizeExperiments(
  by = c("job.id", "algorithm", "problem"))

# Submit all experiments to your local system  
submitJobs(tab)

# Alternatively, submit them to the linux cluster with respective resources
# source("benchmarks/resources.R")
# submitJobs(tab, resources = resources.serial.default)
```

We again store the compromised results. 

```{r eval=FALSE}
reg = loadRegistry("regs/mlp_bo_registry")

# Check status of the experiments 
getStatus()

source("benchmarks/mlp/2_mbo_runs/reduce.R")
reduce_results_mlrmbo(reg, savedir = "data/runs/mlp_results")
```


### (3) Compute the Ground-Truth PDPs 

We compute the ground-truth PDP on the data in order to be able to compare our estimates against it. 

We will use the same data for Monte Carlo Sampling as well as the same set of grid points for both estimates. 

```{r}
source("benchmarks/mlp/3_gt_pdp/generate_data.R")
```


```{r eval=FALSE}
reg = loadRegistry("regs/mlp_ground_truth_pdp", writeable = TRUE)

# Overview table over experiments to be run
tab = summarizeExperiments(
  by = c("job.id", "algorithm", "problem"))

# Submit all experiments to your local system  
submitJobs(tab[1, ])

# Alternatively, submit them to the linux cluster with respective resources
# source("benchmarks/resources.R")
# submitJobs(tab, resources = resources.serial.default)
```

We again store the compromised results. 

```{r eval=FALSE}
reg = loadRegistry("regs/mlp_ground_truth_pdp")

# Check status of the experiments 
getStatus()

source("benchmarks/mlp/3_gt_pdp/reduce.R")
reduce_results_gt_pdp(reg, savedir = "data/runs/mlp_results")
```


### (4) Compute Tree-Partitioning and perform Evaluation

Finally, we perform the tree-partitioning.

```{r}
source("benchmarks/mlp/4_tree_splitting/generate_data.R")
```


```{r eval=FALSE}
reg = loadRegistry("regs/tree_splitting", writeable = TRUE)

# Overview table over experiments to be run
tab = summarizeExperiments(
  by = c("job.id", "algorithm", "problem", "lambda", "objective"))

# Submit all experiments to your local system  
submitJobs(tab[1, ])

# Alternatively, submit them to the linux cluster with respective resources
# source("benchmarks/resources.R")
# submitJobs(tab, resources = resources.serial.default)
```

We again store the compromised results. 

```{r eval=FALSE}
reg = loadRegistry("regs/tree_splitting")

# Check status of the experiments 
getStatus()

source("benchmarks/mlp/4_tree_splitting/reduce.R")
reduce_trees(reg, savedir = "data/runs/mlp_results")
```
