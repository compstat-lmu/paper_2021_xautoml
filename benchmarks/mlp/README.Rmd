---
output: rmarkdown::github_document
---

```{r setup}
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
```


# Experiments on a DNN Surrogate Benchmark (Section 6.2)

The setup of this experiment is slightly more complicated, since we first have to train an empirical performance model, before we can perform the MBO runs and perform the tree splitting. 


### Training the Empirical Performance Model 

In order to train the empirical performance model, the LCBench data needs first to be downloaded from [here](https://ndownloader.figshare.com/files/21188598) and unzipped. 

Then, the following `get_data_from_LCBench.py` allows to store the respective datasets into a .csv files in a proper directory structure. The python script was tested with Python 3.7.6. 

As a result, you will find the LCBench data stored in `data/runs/mlp/<dataset>/0_objective/lcbench2000.csv`. 

Then, the following script allows will again setup the experimental registry in which the surrogates are computed. 

```{r}
source("benchmarks/mlp/1_empirical_performance_model/generate_data.R")
```

The above script create an experimental registry in `regs/LCBench_surogate_registry`. 

The core part, which is the actual definition of the experiment is in the function `randomsearch(...)` in the file `benchmarks/synthetic/config.R`. In this function, a basic random search is performed to find a proper configuration of a random forest, and fit a random forest with this configuration. 

Jobs are submitted as follows. 

```{r}
reg = loadRegistry("regs/LCBench_surogate_registry", writeable = TRUE)

# Overview table over experiments to be run
tab = summarizeExperiments(
  by = c("job.id", "algorithm", "problem"))

# Submit all experiments to your local system  
submitJobs(tab)

# Alternatively, submit them to the linux cluster with respective resources
# source("benchmarks/resources.R")
# submitJobs(tab, resources = resources.serial.default)
```




### Run MBO 



### Perform the Tree Splitting 
